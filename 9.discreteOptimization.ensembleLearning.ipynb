{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dimod\n",
    "from qiskit.quantum_info import Pauli\n",
    "from qiskit_aqua import Operator\n",
    "from qiskit_aqua import get_aer_backend, QuantumInstance\n",
    "from qiskit_aqua.algorithms import QAOA\n",
    "from qiskit_aqua.components.optimizers import COBYLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any learning algorithm will always have strengths and weaknesses: a single model is unlikely to fit every possible scenario. Ensembles combine multiple models to achieve higher generalization performance than any of the constituent models is capable of. How do we assemble the weak learners? We can use some sequential heuristics. For instance, given the current collection of models, we can add one more based on where that particular model performs well. Alternatively, we can look at all the correlations of the predictions between all models, and optimize for the most uncorrelated predictors. Since this latter is a global approach, it naturally maps to a quantum computer. But first, let's take a look a closer look at loss functions and regularization, two key concepts in machine learning.\n",
    "\n",
    "# 1) Loss Functions and Regularization\n",
    "\n",
    "If you can solve a problem by a classical computer -- let that be a laptop or a massive GPU cluster -- there is little value in solving it by a quantum computer that costs ten million dollars. The interesting question in quantum machine learning is whether there are problems in machine learning and AI that fit quantum computers naturally, but are challenging on classical hardware. This, however, requires a good understanding of both machine learning and contemporary quantum computers.\n",
    "\n",
    "In this course, we primarily focus on the second aspect, since there is no shortage of educational material on classical machine learning. However, it is worth spending a few minutes on going through some basics.\n",
    "\n",
    "Let us take a look at the easiest possible problem: the data points split into two, easily distinguishable sets. We randomly generate this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c1 = np.random.rand(50, 2) / 5\n",
    "c2 = (-0.6, 0.5) + np.random.rand(50, 2) / 5\n",
    "\n",
    "data = np.concatenate((c1, c2))\n",
    "labels = np.array([0] * 50 + [1] *50)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(111, xticks=[], yticks=[])\n",
    "plt.scatter(data[:50, 0], data[:50, 1], color='navy')\n",
    "plt.scatter(data[50:, 0], data[50:, 1], color='c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split\n",
    "\n",
    "Let's shuffle the data set into a training set that we are going to optimize over (2/3 of the data), and a test set where we estimate our generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(labels))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# train on a random 2/3 and test on the remaining 1/3\n",
    "\n",
    "idx_train, idx_test = idx[:2 * len(idx) // 3], idx[2 * len(idx) // 3:]\n",
    "\n",
    "X_train, X_test = data[idx_train], data[idx_test]\n",
    "y_train, y_test = labels[idx_train], labels[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the package `scikit-learn` to train various machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Let's train a perceptron (basically similar to logistic regression but where the activation function is a simple unit step).  This is a **linear model** whose loss function is defined by $\\frac{1}{N}\\sum_{i=1}^N |h(x_i)-y_i)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  1.00\n",
      "accuracy (test):  1.00\n"
     ]
    }
   ],
   "source": [
    "model_1 = Perceptron(max_iter=1000, tol=1e-3)\n",
    "model_1.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does a great job. It is a linear model, meaning its decision surface is a plane. \n",
    "\n",
    "## Support Vector Classification\n",
    "\n",
    "Our dataset is separable by a plane, so let's try another linear model, but this time a support vector machine. If you eyeball our dataset, you will see that to define the separation between the two classes, actually only a few points close to the margin are relevant. These are called support vectors and support vector machines aim to find them. Its objective function measures the loss and it has a regularization term with a weight $C$. The $C$ hyperparameter controls a regularization term that penalizes the objective for the number of support vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  1.00\n",
      "accuracy (test):  1.00\n",
      "Number of support vectors: 12\n"
     ]
    }
   ],
   "source": [
    "model_2 = SVC(kernel='linear', C=1)\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
    "print('Number of support vectors:', sum(model_2.n_support_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.52\n",
      "accuracy (test):  0.47\n",
      "Number of support vectors: 64\n"
     ]
    }
   ],
   "source": [
    "model_2 = SVC(kernel='linear', C=0.01)\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
    "print('Number of support vectors:', sum(model_2.n_support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model gets confused by using two many datapoints in the final classifier. This is one example where regularization helps.\n",
    "\n",
    "# 2) Ensemble methods\n",
    "\n",
    "Ensembles yield better results when there is considerable diversity among the base classifiers. If diversity is sufficient, base classifiers make different errors, and a strategic combination may reduce the total error, ideally improving generalization performance. A constituent model in an ensemble is also called a base classifier or weak learner, and the composite model a strong learner.\n",
    "\n",
    "The generic procedure of ensemble methods has two steps. First, develop a set of base classifiers from the training data. Second, combine them to form the ensemble. In the simplest combination, the base learners vote, and the label prediction is based on majority. More involved methods weigh the votes of the base learners. \n",
    "\n",
    "**We define our figure of merit as accuracy in a balanced dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data, labels = sklearn.datasets.make_circles()\n",
    "idx = np.arange(len(labels))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# train on a random 2/3 and test on the remaining 1/3\n",
    "\n",
    "idx_train, idx_test = idx[: 2 * len(idx) // 3], idx[2 * len(idx) // 3:]\n",
    "\n",
    "X_train, X_test = data[idx_train], data[idx_test]\n",
    "\n",
    "# binary -> spin\n",
    "y_train, y_test = 2 * labels[idx_train] - 1, 2 * labels[idx_test] - 1\n",
    "\n",
    "scaler, normalizer = sklearn.preprocessing.StandardScaler(), sklearn.preprocessing.Normalizer()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_test = normalizer.fit_transform(X_test)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(111, xticks=[], yticks=[])\n",
    "plt.scatter(data[labels == 0, 0], data[labels == 0, 1], color='navy')\n",
    "plt.scatter(data[labels == 1, 0], data[labels == 1, 1], color='c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Let's train a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.58\n",
      "accuracy (test):  0.38\n"
     ]
    }
   ],
   "source": [
    "model_1 = Perceptron(max_iter=1000, tol=1e-3)\n",
    "model_1.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its decision surface is linear, we get a poor accuracy. Would a support vector machine with a nonlinear kernel fare better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "Radial basis function kernel.\n",
    "\n",
    "**!!!changing [gamma parameter](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from auto to scale seems to have a VERY strong influence on further results!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.64\n",
      "accuracy (test):  0.24\n"
     ]
    }
   ],
   "source": [
    "model_2 = SVC(kernel='rbf', gamma='auto')\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs better on the training set, but at the cost of extremely poor generalization. \n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Boosting is an ensemble method that explicitly seeks models that complement one another. The variation between boosting algorithms is how they combine weak learners. Adaptive boosting (AdaBoost) is a popular method that combines the weak learners in a sequential manner based on their individual accuracies. It has a convex objective function that does not penalize for complexity: it is likely to include all available weak learners in the final ensemble. Let's train AdaBoost with a few weak learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.65\n",
      "accuracy (test):  0.29\n"
     ]
    }
   ],
   "source": [
    "model_3 = AdaBoostClassifier(n_estimators=3)\n",
    "model_3.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_3.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_3.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its performance is marginally better than that of the SVM.\n",
    "\n",
    "# 3) QBoost\n",
    "\n",
    "The idea of Qboost is that optimization on a quantum computer is not constrained to convex objective functions, therefore we can add arbitrary penalty terms and rephrase our objective [[1](#1)]. Qboost solves the following problem:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}w_kh_k(x_i)-\n",
    "y_i\\right)^2+\\lambda\\|w\\|_0\\right),\n",
    "$$\n",
    "\n",
    "where $h_k(x_i)$ is the prediction of (weak) classifier learner indexed by $k$ applied to the training instance $i$ and $y_i$ is the corresponding ground-truth label.  In other words, for each training instance $i \\in [1, \\cdots, N]$, we are summing up all the L2 losses defined by the ensemble of classifiers $k \\in [1, \\cdots, K]$.   **The weights $w_k$ are binary so this objective function maps to an Ising model**. The regularization in the $l_0$ norm ensures sparsity.\n",
    "\n",
    "Let us expand the quadratic part of the objective:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\n",
    "\\left( \\left(\\sum_{k=1}^{K} w_k h_k(x_i)\\right)^{2} -\n",
    "2\\sum_{k=1}^{K} w_k h_k(x_i)y_i + y_i^{2}\\right) + \\lambda \\|w\\|_{0}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Because we are considering a binary classifier, $y_i^{2}$ is a constant offset that does not change the nature of the optimization problem which can be reduced to:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\n",
    "\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{l=1}^{K} w_k w_l\n",
    "\\left(\\sum_{i=1}^{N}h_k(x_i)h_l(x_i)\\right) - \n",
    "\\frac{2}{N}\\sum_{k=1}^{K}w_k\\sum_{i=1}^{N} h_k(x_i)y_i +\n",
    "\\lambda \\|w\\|_{0} \\right).\n",
    "$$\n",
    "\n",
    "This form shows that we consider all correlations between the predictions of the weak learners: there is a summation of $h_k(x_i)h_l(x_i)$. Since this term has a positive sign, **we penalize for correlations**. On the other hand, the correlation with the true label, $h_k(x_i)y_i$, has a negative sign. The regularization term remains unchanged.\n",
    "\n",
    "Let us consider all three models from the previous section as weak learners:\n",
    "- model_1 / perceptron\n",
    "- model_2 / support vector machine (RBF kernel)\n",
    "- model_3 / AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3]\n",
    "\n",
    "n_models = len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are scaled to be in the set $\\{-1/\\text{n_models}, 1/\\text{n_models}\\}$ to reflect the averaging in the objective (**see reference paper for details**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([h.predict(X_train) for h in models], dtype=np.float64)\n",
    "\n",
    "predictions *= 1/n_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can verify the training errors reported earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1 = 0.58\n",
      "model_2 = 0.64\n",
      "model_3 = 0.65\n"
     ]
    }
   ],
   "source": [
    "modAcc = lambda pred: sum([np.sign(x) == np.sign(y) for x, y in zip(pred, y_train)]) / len(y_train)\n",
    "mod1Acc = modAcc(predictions[0])\n",
    "mod2Acc = modAcc(predictions[1])\n",
    "mod3Acc = modAcc(predictions[2])\n",
    "print('model_1 = %.2f\\nmodel_2 = %.2f\\nmodel_3 = %.2f' % (mod1Acc, mod2Acc, mod3Acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Solving by simulated annealing\n",
    "\n",
    "First, let us start by setting $\\lambda = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the quadratic binary optimization of the objective function as we expanded above.  First of all, the off-diagonal coupling terms $w_k w_l$ are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.dot(predictions, predictions.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to correct the diagonal terms (i.e. those that contain only a single $w_k$) corresponding to the linear part of the optimization problem.  Note that the first term also has a linear contribution when $k=l$.  This is because of the binary property of the weights and the classes such that:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_k w_k &= w_k \\\\\n",
    "h_k(x_i) h_k(x_i) &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Those diagonal components are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wii = len(X_train) / (n_models ** 2) + λ - 2 * np.dot(predictions, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which we can fill all the components of $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[np.diag_indices_from(w)] = wii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we encode the coupling terms as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.666666666666667,\n",
       " (0, 1): 2.4444444444444446,\n",
       " (0, 2): 2.2222222222222223,\n",
       " (1, 1): -3.666666666666666,\n",
       " (1, 2): 4.444444444444444,\n",
       " (2, 2): -5.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = {(i, j): w[i, j] for i in range(n_models) for j in range(i, n_models)}\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to solve the quadratic binary optimization with simulated annealing and read out the optimal weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dimod.SimulatedAnnealingSampler()\n",
    "response = sampler.sample_qubo(W, num_reads=10)\n",
    "weights = list(response.first.sample.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only AdaBoost made it to the final ensemble. The first two models perform poorly and their predictions are correlated.\n",
    "\n",
    "If you remove regularization by setting $\\lambda=0$ above, the second model also enters the ensemble, decreasing overall performance. This shows that the regularization is in fact important.\n",
    "\n",
    "## B) Solving by QAOA (Quantum Approximate Optimization Algorithm)\n",
    "\n",
    "**Since our problem is just an Ising model, we can also solve it on a gate-model quantum computer by QAOA**. Let us explicitly map the binary optimization to the Ising model using the ``qubo_to_ising`` [method](https://docs.ocean.dwavesys.com/projects/dimod/en/latest/reference/generated/dimod.utilities.qubo_to_ising.html) provided by D-Wave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = {0: 2.0, 1: -0.11111111111111094, 2: -0.8333333333333335}\n",
      "J = {(0, 1): 0.6111111111111112, (0, 2): 0.5555555555555556, (1, 2): 1.111111111111111}\n",
      "constant offset applied to energy = -1.22\n"
     ]
    }
   ],
   "source": [
    "h, J, offset = dimod.qubo_to_ising(W)\n",
    "\n",
    "print('h = %s' % h)\n",
    "print('J = %s' % J)\n",
    "print('constant offset applied to energy = %.2f' % offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to translate the Ising couplings to be suitable for solving by the QAOA routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = w.shape[0]\n",
    "\n",
    "Xvector = np.zeros(num_nodes)\n",
    "\n",
    "def logicVec(vecSize, *indices):\n",
    "    initVec = np.zeros(vecSize)\n",
    "    for index in indices:\n",
    "        assert index < vecSize\n",
    "        initVec[index] = 1\n",
    "    return initVec\n",
    "\n",
    "diagonal = [[h[i], Pauli(logicVec(num_nodes, i), Xvector)] for i in h]\n",
    "offDiagonal = [[J[i, j], Pauli(logicVec(num_nodes, i, j), Xvector)] for i, j in J]\n",
    "\n",
    "# note that the ORDER in the list is important\n",
    "\n",
    "# ising_model = Operator(paulis = diagonal + offDiagonal)\n",
    "ising_model = Operator([diagonal[0], offDiagonal[0], offDiagonal[1], diagonal[1], offDiagonal[2], diagonal[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the optimization using the QAOA implementation provided by D-Wave (instead of our own as done in the previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaoa = QAOA(ising_model, optimizer = COBYLA(), p=1, operator_mode='matrix')\n",
    "quantum_instance = QuantumInstance(get_aer_backend('statevector_simulator'), shots=100)\n",
    "result = qaoa.run(quantum_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract the most likely solution; **I'm hoping, as Peter does, that this is self-explanatory :-) ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.argmax(result['eigvecs'][0])\n",
    "weights = np.zeros(num_nodes)\n",
    "for i in range(num_nodes):\n",
    "    weights[i] = k % 2\n",
    "    k >>= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtain the following set of weights to assign to all the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply this model ensemble to the datasets and measure the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5757575757575758\n",
      "0.4411764705882353\n"
     ]
    }
   ],
   "source": [
    "def modelPrediction(modelID, dataSet):\n",
    "    modelPrediction = weights[modelID] * models[modelID].predict(dataSet)\n",
    "    avgPrediction = np.sum(modelPrediction) / len(dataSet)\n",
    "    return {'modelPrediction': modelPrediction, 'avgPrediction': avgPrediction}\n",
    "\n",
    "def ensembleRes(dataSet):\n",
    "    ensemblePreds = [modelPrediction(modelID, dataSet) for modelID in range(n_models)]\n",
    "    avgPred = sum([pred['avgPrediction'] for pred in ensemblePreds]) / n_models\n",
    "    accumulatePred = sum([pred['modelPrediction'] for pred in ensemblePreds])\n",
    "    return np.sign(accumulatePred - avgPred)\n",
    "\n",
    "print(metric(ensembleRes(X_train), y_train))\n",
    "print(metric(ensembleRes(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the generalization gap is indeed better and that we have improved the accuracy on the test set.\n",
    "\n",
    "# References\n",
    "\n",
    "[1] Neven, H., Denchev, V.S., Rose, G., Macready, W.G. (2008). [Training a binary classifier with the quantum adiabatic algorithm](https://arxiv.org/abs/0811.0416). *arXiv:0811.0416*.  <a id='1'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
